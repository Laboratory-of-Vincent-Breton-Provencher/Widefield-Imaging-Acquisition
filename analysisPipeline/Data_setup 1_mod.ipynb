{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17776bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "import sys  \n",
    "import tifffile as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import cv2\n",
    "from scipy.signal import butter, filtfilt, detrend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = r\"C:\\Users\\alexc\\Documents\\GitHub\\Wide-Brain\"\n",
    "current_path = os.getcwd()\n",
    "sys.path.insert(1, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3599a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from WFmovie_mod import WFmovie\n",
    "from WFmovie_mod import create_channel\n",
    "from WFmovie_mod import ioi_epsilon_pathlength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ebc7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_folders(path, keywords):\n",
    "    initial_folders = [f.path for f in os.scandir(path) if f.is_dir()]\n",
    "    folders = []\n",
    "    for folder in initial_folders:\n",
    "        if all(keyword in folder for keyword in keywords):\n",
    "            folders.append(folder)\n",
    "    for i in range(len(folders)):\n",
    "        folders[i] += '/'\n",
    "    return folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db34b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_files(path, keywords):\n",
    "    items = os.listdir(path)\n",
    "    files = []\n",
    "    for item in items:\n",
    "        if all(keyword in item for keyword in keywords):\n",
    "            files.append(item)\n",
    "    files.sort()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc348878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regress_timeseries(y, x, intercept=False, offset=False):\n",
    "    \"\"\" Regress out the linear contribution of a regressor on time series data\n",
    "    Args:\n",
    "        y (ndarray): Time series specified as Nx1 vector. For M time series, specify as NxM array. \n",
    "        x (1D vector): NX1 time series to regress out of data.\n",
    "        intercept (default = True) Specify wether or not to include an intercept term in the regressor.\n",
    "        offset (default = False) Option to re-offset the regressed data at the estimated intercept\n",
    "    Returns:\n",
    "        eps (ndarray): Regressed data\n",
    "    \"\"\"\n",
    "    if np.ndim(y)==1: #Make sure time series are presented as NX1 vectors\n",
    "        y = y[:,None]\n",
    "    N = np.shape(y)[0]\n",
    "    if np.shape(x)[0] != N:\n",
    "        raise Exception('Regressor must be of height N, with N the number of time points.')\n",
    "    if np.ndim(x)==1: \n",
    "        x = x[:,None]\n",
    "    if (offset and intercept) is False:\n",
    "        intercept = True\n",
    "    if intercept:\n",
    "        x = np.insert(x,0,np.ones((1,N)),axis = 1)\n",
    "    x_inv = np.linalg.pinv(x)\n",
    "    beta = np.matmul(x_inv,y)\n",
    "    y_est = np.matmul(x,beta)\n",
    "    eps = y - y_est + np.mean(y)\n",
    "    if offset:\n",
    "        eps = eps+np.matmul(x[:,0][:,None],beta[0,:][None,:])\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f4927d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_pixels(frame, bin_size):\n",
    "    height, width = frame.shape[:2]\n",
    "    binned_height = height // bin_size\n",
    "    binned_width = width // bin_size\n",
    "\n",
    "    reshaped_frame = frame[:binned_height * bin_size, :binned_width * bin_size].reshape(binned_height, bin_size, binned_width, bin_size)\n",
    "    binned_frame = np.sum(reshaped_frame, axis=(1, 3), dtype=np.float32)\n",
    "    binned_frame = binned_frame / (bin_size**2)\n",
    "\n",
    "    return binned_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1729d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_1404\\1957387799.py:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  with tf.TiffWriter(output_path+\"\\dHbO.tif\", bigtiff=True) as writerHbO, tf.TiffWriter(output_path+\"\\dHbR.tif\", bigtiff=True) as writerHbR, tf.TiffWriter(output_path+\"\\dHbT.tif\", bigtiff=True) as writerHbT:\n",
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_1404\\1957387799.py:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  with tf.TiffWriter(output_path+\"\\dHbO.tif\", bigtiff=True) as writerHbO, tf.TiffWriter(output_path+\"\\dHbR.tif\", bigtiff=True) as writerHbR, tf.TiffWriter(output_path+\"\\dHbT.tif\", bigtiff=True) as writerHbT:\n",
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_1404\\1957387799.py:23: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  with tf.TiffWriter(output_path+\"\\dHbO.tif\", bigtiff=True) as writerHbO, tf.TiffWriter(output_path+\"\\dHbR.tif\", bigtiff=True) as writerHbR, tf.TiffWriter(output_path+\"\\dHbT.tif\", bigtiff=True) as writerHbT:\n"
     ]
    }
   ],
   "source": [
    "def convert_to_hb(path_green, path_red, output_path):\n",
    "    with tf.TiffFile(path_green) as tifG, tf.TiffFile(path_red) as tifR:\n",
    "        R_green = tifG.pages\n",
    "        R_red = tifR.pages\n",
    "    \n",
    "        num_frames = len(R_green) # Assuming the frames are along the first dimension\n",
    "        frame_shape = R_green[0].asarray().shape\n",
    "        stack_shape = (num_frames, frame_shape[0], frame_shape[1])\n",
    "\n",
    "        lambda1 = 450 #nm\n",
    "        lamba2 = 700 #nm\n",
    "        npoints = 1000\n",
    "        baseline_hbt = 100 #uM\n",
    "        baseline_hbo = 60 #uM\n",
    "        baseline_hbr = 40 #uM\n",
    "        rescaling_factor = 1e6\n",
    "        \n",
    "        os.chdir(r\"C:\\Users\\alexc\\Documents\\GitHub\\Wide-Brain\")\n",
    "        eps_pathlength = ioi_epsilon_pathlength(lambda1, lamba2, npoints, baseline_hbt, baseline_hbo, baseline_hbr, filter=None)\n",
    "        Ainv = np.linalg.pinv(eps_pathlength)*rescaling_factor\n",
    "        os.chdir(current_path)\n",
    "\n",
    "        with tf.TiffWriter(output_path+\"\\dHbO.tif\", bigtiff=True) as writerHbO, tf.TiffWriter(output_path+\"\\dHbR.tif\", bigtiff=True) as writerHbR, tf.TiffWriter(output_path+\"\\dHbT.tif\", bigtiff=True) as writerHbT:\n",
    "            for i in range(num_frames):\n",
    "                data_G = R_green[i].asarray()\n",
    "                data_R = R_red[i].asarray()\n",
    "                ln_green = -np.log(data_G.flatten())\n",
    "                ln_red = -np.log(data_R.flatten())\n",
    "                ln_R = np.concatenate((ln_green.reshape(1, len(ln_green)), ln_red.reshape(1, len(ln_green))))\n",
    "                Hbs = np.matmul(Ainv, ln_R)\n",
    "                d_HbO = Hbs[0].reshape(frame_shape)\n",
    "                d_HbR = Hbs[1].reshape(frame_shape)\n",
    "                np.nan_to_num(d_HbO, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                np.nan_to_num(d_HbR, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                d_HbT = d_HbO + d_HbR\n",
    "                \n",
    "                writerHbO.write(np.float32(d_HbO), contiguous=True)\n",
    "                writerHbR.write(np.float32(d_HbR), contiguous=True)\n",
    "                writerHbT.write(np.float32(d_HbT), contiguous=True)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad37518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speckle_contrast(raw_speckle_data, window_size=7):\n",
    "    num_frames, height, width = raw_speckle_data.shape\n",
    "    contrast_data = np.zeros((num_frames, height, width))\n",
    "\n",
    "    for frame_idx in range(num_frames):\n",
    "        frame_data = raw_speckle_data[frame_idx]\n",
    "\n",
    "        # Calculate local mean and standard deviation for the current frame\n",
    "        local_mean = ndi.uniform_filter(frame_data, size=window_size)\n",
    "        local_variance = ndi.uniform_filter(frame_data**2, size=window_size) - local_mean**2\n",
    "        local_std = np.sqrt(local_variance)\n",
    "        \n",
    "        # Calculate speckle contrast for the current frame\n",
    "        contrast_data[frame_idx] = local_std / local_mean\n",
    "\n",
    "    return contrast_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d58a55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_filter(data_3d, lowcut, highcut, fs=3.0, order=5):\n",
    "    \"\"\"\n",
    "    Apply a band-pass filter to a 3D array along the 0th axis using vectorized operations and renormalize the amplitude.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_3d: the input 3D array (time, x, y)\n",
    "    - lowcut: low cut-off frequency (default is 0.008 Hz)\n",
    "    - highcut: high cut-off frequency (default is 0.09 Hz)\n",
    "    - fs: sampling frequency (default is 3 Hz)\n",
    "    - order: order of the filter (default is 5)\n",
    "    \n",
    "    Returns:\n",
    "    - Filtered and renormalized 3D array.\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    \n",
    "    # Compute the max values of the original data for each pixel\n",
    "    max_values = np.max(np.abs(data_3d), axis=0)\n",
    "    \n",
    "    # Reshape the 3D array into a 2D array\n",
    "    data_2d = data_3d.reshape(data_3d.shape[0], -1)\n",
    "    \n",
    "    # Apply the filter to the entire 2D array along the 0th axis\n",
    "    filtered_data_2d = filtfilt(b, a, data_2d, axis=0)\n",
    "    \n",
    "    # Reshape the 2D array back to its original 3D shape\n",
    "    filtered_data_3d = filtered_data_2d.reshape(data_3d.shape)\n",
    "    \n",
    "    # Renormalize the amplitude for each pixel's time series\n",
    "    max_values_filtered = np.max(np.abs(filtered_data_3d), axis=0)\n",
    "    mask = max_values_filtered != 0  # Avoid division by zero\n",
    "    filtered_data_3d[:, mask] *= (max_values[mask] / max_values_filtered[mask])\n",
    "    \n",
    "    return filtered_data_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f518b0",
   "metadata": {},
   "source": [
    "## dHb pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc3400ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_IOI(folder_path, save_path, red=True, green=True, baseline=[0, -1], bin_pixel=None, regression=False, filter_IOI=False):\n",
    "    if red:\n",
    "        # Create red channel and IOI_red.tif\n",
    "        path_red = save_path + r\"\\IOI_red.tif\"\n",
    "\n",
    "        if len(identify_files(folder_path, [\"rawdata_red\"])) == 0:\n",
    "            if len(identify_files(folder_path, [\"redChan\"])) == 0:\n",
    "                create_channel(folder_path=folder_path, channel=\"red\")\n",
    "            data = np.load(folder_path+r\"\\redChan.npy\").astype(np.float32)\n",
    "            os.remove(folder_path+r\"\\redChan.npy\")\n",
    "            data /= 4096.0\n",
    "            data = np.flip(data, axis=(1,2))\n",
    "            tf.imwrite(folder_path + r\"\\rawdata_red.tif\", data)\n",
    "        elif len(identify_files(folder_path, [\"rawdata_red\"])) != 0:\n",
    "            data = tf.imread(folder_path + r\"\\rawdata_red.tif\")\n",
    "\n",
    "        data = ndi.median_filter(data, size=[5, 1, 1]) \n",
    "\n",
    "        if regression:\n",
    "            print(\"Global regression\")\n",
    "            regressor = np.mean(data, axis=(1,2))\n",
    "            T, N, M = data.shape\n",
    "            time_series = data.transpose(1, 2, 0).reshape(N * M, T)\n",
    "            result = [regress_timeseries(series, regressor) for series in time_series]\n",
    "            data = np.array(result).reshape(N, M, T).transpose(2, 0, 1)\n",
    "            result, timeseries, regressor = None, None, None  # release memory\n",
    "\n",
    "        norm_factor = np.mean(data[baseline[0]:baseline[1]], axis=0)\n",
    "        if bin_pixel is not None:\n",
    "            norm_factor = bin_pixels(norm_factor, bin_pixel)\n",
    "\n",
    "        if filter_IOI:\n",
    "            print(\"Filtering\")\n",
    "            data = bandpass_filter(data, lowcut=0.008, highcut=0.09, fs=3.0, order=3)\n",
    "\n",
    "        print(\"Saving data\")\n",
    "        with tf.TiffWriter(path_red, bigtiff=True) as writer:\n",
    "            for frame in data:\n",
    "                if bin_pixel is not None:\n",
    "                    frame = bin_pixels(frame, bin_pixel)\n",
    "                frame = frame / norm_factor\n",
    "                writer.write(np.float32(frame), contiguous=True)\n",
    "        data = None\n",
    "    \n",
    "    if green:\n",
    "        # Create green channel and IOI_green.tif\n",
    "        path_green = save_path + r\"\\IOI_green.tif\"\n",
    "\n",
    "        if len(identify_files(folder_path, [\"rawdata_green\"])) == 0:\n",
    "            if len(identify_files(folder_path, [\"greenChan\"])) == 0:\n",
    "                create_channel(folder_path=folder_path, channel=\"green\")\n",
    "            data = np.load(folder_path+r\"\\greenChan.npy\").astype(np.float32)\n",
    "            os.remove(folder_path+r\"\\greenChan.npy\")\n",
    "            data /= 4096.0\n",
    "            data = np.flip(data, axis=(1,2))\n",
    "            tf.imwrite(folder_path + r\"\\rawdata_green.tif\", data)\n",
    "        elif len(identify_files(folder_path, [\"rawdata_green\"])) != 0:\n",
    "            data = tf.imread(folder_path + r\"\\rawdata_green.tif\")\n",
    "\n",
    "        data = ndi.median_filter(data, size=[5, 1, 1]) \n",
    "\n",
    "        if regression:\n",
    "            print(\"Global regression\")\n",
    "            regressor = np.mean(data, axis=(1,2))\n",
    "            T, N, M = data.shape\n",
    "            time_series = data.transpose(1, 2, 0).reshape(N * M, T)\n",
    "            result = [regress_timeseries(series, regressor) for series in time_series]\n",
    "            data = np.array(result).reshape(N, M, T).transpose(2, 0, 1)\n",
    "            result, timeseries, regressor = None, None, None  # release memory\n",
    "\n",
    "        norm_factor = np.mean(data[baseline[0]:baseline[1]], axis=0)\n",
    "        if bin_pixel is not None:\n",
    "            norm_factor = bin_pixels(norm_factor, bin_pixel)\n",
    "\n",
    "        if filter_IOI:\n",
    "            print(\"Filtering\")\n",
    "            data = bandpass_filter(data, lowcut=0.008, highcut=0.09, fs=3.0, order=3)\n",
    "\n",
    "        print(\"Saving data\")\n",
    "        with tf.TiffWriter(path_green, bigtiff=True) as writer:\n",
    "            for frame in data:\n",
    "                if bin_pixel is not None:\n",
    "                    frame = bin_pixels(frame, bin_pixel)\n",
    "                frame = frame / norm_factor\n",
    "                writer.write(np.float32(frame), contiguous=True)\n",
    "        data = None\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a8190bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dHb_pipeline(folder_path, save_path, baseline=[0, -1], bin_pixel=None, regression=False, filter_IOI=False, filter_dHb=False):\n",
    "    # Create red channel and IOI_red.tif\n",
    "    print(\"Load red channel\")\n",
    "    path_red = save_path + r\"\\IOI_red.tif\"\n",
    "    compute_IOI(folder_path, save_path, red=True, green=False, baseline=baseline, bin_pixel=bin_pixel, regression=regression, filter_IOI=filter_IOI)\n",
    "          \n",
    "    # Create green channel and IOI_green.tif\n",
    "    print(\"Load green channel\")\n",
    "    path_green = save_path + r\"\\IOI_green.tif\"\n",
    "    compute_IOI(folder_path, save_path, red=False, green=True, baseline=baseline, bin_pixel=bin_pixel, regression=regression, filter_IOI=filter_IOI)\n",
    "      \n",
    "    # Create dHbO.tif, dHbR.tif and dHbT.tif\n",
    "    print(\"Convert to dHb\")\n",
    "    convert_to_hb(path_green, path_red, save_path)\n",
    "    \n",
    "    if filter_dHb:\n",
    "        print(\"Filtering dHb\")\n",
    "        Hb_paths = [r\"\\dHbO.tif\", r\"\\dHbR.tif\", r\"\\dHbT.tif\"]\n",
    "        for path in Hb_paths:\n",
    "            data = tf.imread(folder_path + path)\n",
    "            data = bandpass_filter(data, lowcut=0.008, highcut=0.09, fs=3.0, order=5)\n",
    "            tf.imwrite(folder_path + path, np.float32(data))\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7df285c1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CVR single file setup\n",
    "setup = False\n",
    "if setup:\n",
    "    day = \"S9\"\n",
    "    mouse = \"RS_M35_S9\"\n",
    "    path = r\"D:\\CVR_dataset\\{}\\Dataset\\{}\".format(day, mouse)\n",
    "    dHb_pipeline(folder_path=path, save_path=path, baseline=[0, -1], bin_pixel=2, regression=True, filter_IOI=False, filter_dHb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d077f4b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CVR multiple files setup\n",
    "setup = False\n",
    "if setup:\n",
    "    all_paths = [r\"D:\\CVR_dataset\\S7\\Dataset\", r\"D:\\CVR_dataset\\S8\\Dataset\", r\"D:\\CVR_dataset\\S9\\Dataset\", r\"D:\\CVR_dataset\\S10\\Dataset\", r\"D:\\CVR_dataset\\S11\\Dataset\"]\n",
    "    for general_path in tqdm(all_paths):\n",
    "        # CVR data\n",
    "        keyword = [\"CVR_M\"]\n",
    "        folders = identify_folders(general_path, keyword)\n",
    "        for path in folders:\n",
    "            print(path)\n",
    "            try:\n",
    "                dHb_pipeline(folder_path=path, save_path=path, baseline=[0, 90], bin_pixel=2, regression=False, filter_IOI=False, filter_dHb=False)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with item {path}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        # RS data\n",
    "        keyword = [\"RS_M\"]\n",
    "        folders = identify_folders(general_path, keyword)\n",
    "        for path in folders:\n",
    "            print(path)\n",
    "            try:\n",
    "                dHb_pipeline(folder_path=path, save_path=path, baseline=[0, -1], bin_pixel=2, regression=False, filter_IOI=False, filter_dHb=False)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred with item {path}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fbb256c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Single file HbT\n",
    "setup = False\n",
    "if setup:\n",
    "    path = r\"D:\\GCaMP_testing2024\\WF\\06-06\\Data\\RS_M210\"\n",
    "    dHb_pipeline(folder_path=path, save_path=path, baseline=[0, -1], bin_pixel=2, regression=False, filter_IOI=False, filter_dHb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805823b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single file IOI\n",
    "setup = False\n",
    "if setup:\n",
    "    path = r\"D:\\GCaMP_testing2024\\WF\\02-21\\GCaMP_testing_v2\"\n",
    "    compute_IOI(folder_path=path, save_path=path, red=False, green=True, baseline=[0, -1], bin_pixel=None, regression=False, filter_IOI=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30763159",
   "metadata": {},
   "source": [
    "## LSCI pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a71102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSCI_pipeline(folder_path, save_path, background_path=None, baseline=None, regression=False, bin_pixel=True, filter_data=True):\n",
    "    print(\"Load ir channel\")\n",
    "    if len(identify_files(folder_path, [\"rawdata_ir\"])) == 0:\n",
    "        if len(identify_files(folder_path, [\"irChan\"])) == 0:\n",
    "            create_channel(folder_path=folder_path, channel=\"ir\")\n",
    "        data = np.load(folder_path+r\"\\irChan.npy\").astype(np.float32)\n",
    "        os.remove(folder_path+r\"\\irChan.npy\")\n",
    "        data /= 4096.0\n",
    "        data = np.flip(data, axis=(1,2))\n",
    "        tf.imwrite(folder_path + r\"\\rawdata_ir.tif\", data)\n",
    "    elif len(identify_files(folder_path, [\"rawdata_ir\"])) != 0:\n",
    "        data = tf.imread(folder_path + r\"\\rawdata_ir.tif\")\n",
    "    \n",
    "    if background_path:\n",
    "        print(\"Background substraction\")\n",
    "        if len(identify_files(background_path, [\"irChan\"])) == 0:\n",
    "            create_channel(folder_path=background_path, channel=channel)\n",
    "        data_bg = np.load(background_path+r\"\\irChan.npy\").astype(np.float32)\n",
    "        background = np.mean(data_bg / 4096.0, axis=0) \n",
    "        data -= background\n",
    "        del data_bg, background\n",
    "    \n",
    "    if regression:\n",
    "        print(\"Regress out\")\n",
    "        regressor = np.mean(data, axis=(1,2))\n",
    "        T, N, M = data.shape\n",
    "        time_series = data.transpose(1, 2, 0).reshape(N * M, T)\n",
    "        del data\n",
    "        result = [regress_timeseries(series, regressor) for series in time_series]\n",
    "        data = np.array(result).reshape(N, M, T).transpose(2, 0, 1)\n",
    "        del result, time_series\n",
    "    \n",
    "    print(\"Compute speckle\")\n",
    "    static_data = np.array([np.mean(data, axis=0)])\n",
    "    static_data = speckle_contrast(static_data)\n",
    "    if bin_pixel:\n",
    "        static_data = bin_pixels(static_data[0], 2)\n",
    "    \n",
    "    data = speckle_contrast(data)\n",
    "    data = 1 / np.power(data, 2)\n",
    "    \n",
    "    if baseline:\n",
    "        norm_factor = np.mean(data[baseline[0]:baseline[1]], axis=0)\n",
    "        data /= norm_factor\n",
    "    \n",
    "    if filter_data:\n",
    "        data = ndi.gaussian_filter1d(data, sigma=2.0, axis=0)\n",
    "    \n",
    "    print(\"Saving to disk\")\n",
    "    with tf.TiffWriter(save_path + r\"\\LSCI.tif\", bigtiff=True) as writer:\n",
    "        for frame in data:\n",
    "            if bin_pixel:\n",
    "                frame = bin_pixels(frame, 2)\n",
    "            writer.write(np.float32(frame), contiguous=True)\n",
    "\n",
    "    tf.imwrite(save_path + r\"\\static_LSCI.tif\", np.float32(static_data))\n",
    "    del data, static_data\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e4073e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single file setup\n",
    "setup = False\n",
    "if setup:\n",
    "    day = \"S5\"\n",
    "    mouse = \"CVR_M34_{}\".format(day)\n",
    "    path = r\"D:\\CVR_dataset\\{}\\Dataset\\{}\".format(day, mouse)\n",
    "    LSCI_pipeline(folder_path=path, save_path=path, background_path=None, baseline=None, bin_pixel=True, filter_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8792039",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup = False\n",
    "if setup:\n",
    "    general_path = r\"D:\\CVR_dataset\\S5\\Dataset\"\n",
    "    keyword = [\"CVR_M\"]\n",
    "    folders = identify_folders(general_path, keyword)\n",
    "    for path in tqdm(folders):\n",
    "        print(path)\n",
    "        background_path = identify_folders(general_path, \"bg_\"+path[-7:-1])[0]\n",
    "        LSCI_pipeline(folder_path=path, save_path=path, background_path=None, baseline=None, bin_pixel=True, filter_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a22c78",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LSCI testing\n",
    "setup = False\n",
    "if setup:\n",
    "    general_path = r\"D:\\LSCI_testingv5\"\n",
    "    keyword = [\"v\"]\n",
    "    folders = identify_folders(general_path, keyword)\n",
    "    background_path = general_path + r\"\\background\"\n",
    "    for path in tqdm(folders):\n",
    "        print(path)\n",
    "        LSCI_pipeline(folder_path=path, save_path=path, background_path=None, baseline=None, bin_pixel=True, filter_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fcaf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single file LSCI\n",
    "setup = False\n",
    "if setup:\n",
    "    path = r\"D:\\GCaMP_testing2024\\WF\\05-14\\Data\\RS_M210_P100\"\n",
    "    LSCI_pipeline(folder_path=path, save_path=path, background_path=None, baseline=None, bin_pixel=True, filter_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb72a8",
   "metadata": {},
   "source": [
    "## Calcium imaging setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bc83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_minfilter(signal, window=300, sigma1=5, sigma2=100, debug=False):\n",
    "    signal_flatstart = np.copy(signal)\n",
    "    signal_flatstart[0] = signal[1]\n",
    "    smooth = ndi.gaussian_filter1d(signal_flatstart, sigma1)\n",
    "    mins = ndi.minimum_filter1d(smooth, window)\n",
    "    baseline = ndi.gaussian_filter1d(mins, sigma2)\n",
    "    if debug:\n",
    "        debug_out = np.asarray([smooth, mins, baseline])\n",
    "        return debug_out\n",
    "    else:\n",
    "        return baseline\n",
    "\n",
    "    \n",
    "def compute_dff_using_minfilter(timeseries, window=200, sigma1=0.1, sigma2=50):\n",
    "    T, M, N = timeseries.shape\n",
    "    dff = np.zeros((T, M, N))\n",
    "    \n",
    "    for m in range(M):\n",
    "        for n in range(N):\n",
    "            pixel_timeseries = timeseries[:, m, n]\n",
    "            \n",
    "            if np.any(pixel_timeseries):\n",
    "                baseline = baseline_minfilter(pixel_timeseries, window=window, sigma1=sigma1, sigma2=sigma2)\n",
    "                dff[:, m, n] = (pixel_timeseries - baseline) / baseline\n",
    "\n",
    "    return dff\n",
    "\n",
    "\n",
    "def hemodynamic_regression(folder_path, data_array):\n",
    "    regressor_array = tf.imread(folder_path+r\"\\IOI_green.tif\")\n",
    "    regressor_array = regressor_array / np.mean(regressor_array, axis=0)\n",
    "    \n",
    "    regressed_data = data_array / regressor_array\n",
    "\n",
    "    return regressed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcium_pipeline(folder_path, save_path, baseline=[0, -1], regression=False, hemo_corr=False, bin_pixel=None, filter_data=True):\n",
    "    print(\"Loading movie\")\n",
    "    channel = 'blue'\n",
    "    path_blue = save_path + r\"\\Calcium_imaging.tif\"\n",
    "    \n",
    "    if len(identify_files(folder_path, [\"rawdata_blue\"])) == 0:\n",
    "        if len(identify_files(folder_path, [\"blueChan\"])) == 0:\n",
    "            create_channel(folder_path=folder_path, channel=\"blue\")\n",
    "        data = np.load(folder_path+r\"\\blueChan.npy\").astype(np.float32)\n",
    "        os.remove(folder_path+r\"\\blueChan.npy\")\n",
    "        data /= 4096.0\n",
    "        data = np.flip(data, axis=(1,2))\n",
    "        tf.imwrite(folder_path + r\"\\rawdata_blue.tif\", data)\n",
    "    elif len(identify_files(folder_path, [\"rawdata_blue\"])) != 0:\n",
    "        data = tf.imread(folder_path + r\"\\rawdata_blue.tif\")\n",
    "        \n",
    "    # --- Test with transcranial data ---\n",
    "#     data = np.float32(tf.imread(folder_path + r\"\\rawdata_gcamp.tif\"))\n",
    "    # ---\n",
    "    \n",
    "    data = ndi.median_filter(data, size=[5, 1, 1]) \n",
    "    \n",
    "    if bin_pixel is not None:\n",
    "        print(\"Pixel binning\")\n",
    "        binned_data = np.empty((data.shape[0], data.shape[1]//2, data.shape[2]//2), dtype=data.dtype)\n",
    "        for i, frame in enumerate(data):\n",
    "            binned_data[i] = bin_pixels(frame, bin_pixel)\n",
    "        data = binned_data\n",
    "        binned_data = None\n",
    "    \n",
    "    if hemo_corr:\n",
    "        print(\"Hemodynamic regression\")\n",
    "        data /= np.mean(data, axis=0)\n",
    "        data = hemodynamic_regression(folder_path, data)\n",
    "    \n",
    "    print(\"delta F/F calculation\")\n",
    "    data = compute_dff_using_minfilter(data, window=200, sigma1=0.9, sigma2=10)\n",
    "        \n",
    "    if regression:\n",
    "        print(\"Global regression\")\n",
    "        regressor = np.mean(data, axis=(1,2))\n",
    "        T, N, M = data.shape\n",
    "        time_series = data.transpose(1, 2, 0).reshape(N * M, T)\n",
    "        data = None  # release memory\n",
    "        result = [regress_timeseries(series, regressor) for series in time_series]\n",
    "        data = np.array(result).reshape(N, M, T).transpose(2, 0, 1)\n",
    "        result, timeseries, regressor = None, None, None  # release memory\n",
    "    \n",
    "    if filter_data:\n",
    "        data = ndi.gaussian_filter1d(data, sigma=0.9, axis=0)\n",
    "        \n",
    "    print(\"Saving data\")\n",
    "    with tf.TiffWriter(path_blue, bigtiff=True) as writer:\n",
    "        for frame in data:\n",
    "            writer.write(np.float32(frame), contiguous=True)\n",
    "    data = None\n",
    "    \n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be04e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test with transcranial data\n",
    "setup = False\n",
    "if setup:\n",
    "    path = r\"D:\\Widefield_calcium_imaging\\Data\\rs-1-10min_2023-07-11\\Geco3310\\Geco3310_rs-1-10min_OI_2023-07-11\"\n",
    "    calcium_pipeline(path, path, baseline=None, bin_pixel=None, filter_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7626a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading movie\n",
      "Pixel binning\n",
      "Hemodynamic regression\n",
      "delta F/F calculation\n",
      "Saving data\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "setup = True\n",
    "if setup:\n",
    "    path = r\"D:\\GCaMP_testing2024\\WF\\06-06\\Data\\RS_M210\"\n",
    "    calcium_pipeline(path, path, regression=False, hemo_corr=True, bin_pixel=2, filter_data=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff0bf52",
   "metadata": {},
   "source": [
    "## Monitoring pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c5938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_grayscale(input_file, output_file, frame_range):\n",
    "    video = cv2.VideoCapture(input_file)\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_range[1] == -1:\n",
    "        frame_range[1] = total_frames\n",
    "    output_file_template = output_file.replace('.tif', '_{}.tif')\n",
    "\n",
    "    current_frame = frame_range[0]\n",
    "    file_counter = 0\n",
    "    max_frame = min(12000, frame_range[1]-frame_range[0])\n",
    "    while current_frame < frame_range[1]:\n",
    "        output_file = output_file_template.format(file_counter)\n",
    "        print(output_file)\n",
    "        with tf.TiffWriter(output_file, bigtiff=True) as tiff_writer:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, current_frame)\n",
    "            for _ in tqdm(range(max_frame)):\n",
    "                ret, frame = video.read()\n",
    "\n",
    "#                 frame = frame[:, 420:1500]\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                frame = np.float32(frame)\n",
    "                tiff_writer.write(frame, contiguous=True)\n",
    "                \n",
    "                current_frame += 1\n",
    "                if current_frame > frame_range[1]:\n",
    "                    break\n",
    "                    \n",
    "            file_counter += 1\n",
    "            \n",
    "    frame = None\n",
    "    video.release()\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def write_frame_difference(input_folder, input_file, output_file, frame_range, binning=False):\n",
    "    video = cv2.VideoCapture(f\"{input_folder}\\\\{input_file}\")\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_range[1] == -1:\n",
    "        frame_range[1] = total_frames - 1\n",
    "\n",
    "    motion_energy = []\n",
    "    frames_to_process = frame_range[1] - frame_range[0] + 1\n",
    "    num_output_files = (frames_to_process + 11999) // 12000\n",
    "    for file_counter in range(num_output_files):\n",
    "        start_frame = frame_range[0] + file_counter * 12000\n",
    "        end_frame = min(frame_range[0] + (file_counter + 1) * 12000 - 1, frame_range[1])\n",
    "        num_frames_in_output = end_frame - start_frame + 1\n",
    "        output_file_template = output_file.replace('.tif', f'_{file_counter}.tif')\n",
    "        print(output_file_template)\n",
    "\n",
    "        with tf.TiffWriter(output_file_template, bigtiff=True) as tiff_writer:\n",
    "            video.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "            ret, frame1 = video.read()\n",
    "            frame1 = cv2.cvtColor(frame1[:, 420:1500], cv2.COLOR_BGR2GRAY)\n",
    "            for current_frame in tqdm(range(start_frame, end_frame)):\n",
    "                ret, frame2 = video.read()\n",
    "                frame2 = cv2.cvtColor(frame2[:, 420:1500], cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                diff_frame = cv2.absdiff(frame2, frame1)\n",
    "                if binning:\n",
    "                    diff_frame = bin_pixels(diff_frame, 2)\n",
    "                tiff_writer.write(np.uint8(diff_frame), contiguous=True)\n",
    "                motion_energy.append(np.mean(diff_frame))\n",
    "                \n",
    "                frame1 = frame2\n",
    "\n",
    "    video.release()\n",
    "#     np.save(f\"{input_folder}\\\\motion_energy\", np.array(motion_energy))   \n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def compute_motion_energy(input_folder, input_file, frame_range):\n",
    "    video = cv2.VideoCapture(f\"{input_folder}\\\\{input_file}\")\n",
    "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_range[1] == -1:\n",
    "        frame_range[1] = total_frames - 1\n",
    "\n",
    "    motion_energy = []\n",
    "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_range[0])\n",
    "    ret, frame1 = video.read()\n",
    "    frame1 = cv2.cvtColor(frame1[:, 420:1500], cv2.COLOR_BGR2GRAY)\n",
    "    for current_frame in tqdm(range(frame_range[0], frame_range[1])):\n",
    "        ret, frame2 = video.read()\n",
    "        frame2 = cv2.cvtColor(frame2[:, 420:1500], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        diff_frame = cv2.absdiff(frame2, frame1)\n",
    "        motion_energy.append(np.mean(diff_frame))\n",
    "\n",
    "        frame1 = frame2\n",
    "\n",
    "    video.release()\n",
    "    np.save(f\"{input_folder}\\\\motion_energy\", np.array(motion_energy))   \n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def remove_artefacts(input_folder, threshold_lo):\n",
    "    motion_energy_path = os.path.join(input_folder, 'motion_energy.npy')\n",
    "    if os.path.exists(motion_energy_path):\n",
    "        motion_energy = np.load(motion_energy_path)\n",
    "\n",
    "        for idx, value in enumerate(motion_energy):\n",
    "            if value < threshold_lo:\n",
    "                if idx == 0:\n",
    "                    motion_energy[idx] = (value + motion_energy[idx + 1]) / 2\n",
    "                elif idx == len(motion_energy) - 1:\n",
    "                    motion_energy[idx] = (motion_energy[idx - 1] + value) / 2\n",
    "                else:\n",
    "                    motion_energy[idx] = (motion_energy[idx - 1] + motion_energy[idx + 1]) / 2\n",
    "        \n",
    "        motion_energy = ndi.median_filter(motion_energy, size=5)\n",
    "        np.save(motion_energy_path, motion_energy)\n",
    "        motion_energy = None\n",
    "    else:\n",
    "        print(f\"No 'motion_energy.npy' found in {input_folder}\")\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith('.tif'):\n",
    "            input_file = os.path.join(input_folder, file_name)\n",
    "\n",
    "            with tf.TiffFile(input_file) as tif:\n",
    "                frames = tif.asarray()\n",
    "            \n",
    "            for idx, frame in enumerate(frames):\n",
    "                frame_mean = np.mean(frame)\n",
    "                \n",
    "                if frame_mean < threshold_lo:\n",
    "                    if idx == 0:\n",
    "                        frames[idx] = (frame + frames[idx + 1]) // 2\n",
    "                    elif idx == len(frames) - 1:\n",
    "                        frames[idx] = (frames[idx - 1] + frame) // 2\n",
    "                    else:\n",
    "                        frames[idx] = (frames[idx - 1] + frames[idx + 1]) // 2\n",
    "            tf.imwrite(input_file, frames)\n",
    "            frames = None\n",
    "            \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4eae5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "setup = False\n",
    "if setup:\n",
    "    day = \"S10\"\n",
    "    dataset = r\"CVR_M33_{}\".format(day)\n",
    "    folder_path = r\"D:\\CVR_dataset\\{}\\Monitoring\\{}_video\".format(day, dataset)\n",
    "    output_file = folder_path + r\"\\{}_monitoring.tif\".format(dataset)\n",
    "#     indices = np.load(folder_path+r\"\\indices.npy\")\n",
    "#     start = np.where(np.diff(indices) == 1.0)[0][0]\n",
    "#     print(start)\n",
    "#     frame_range = [start, start+int(12.5*60*30)]\n",
    "    frame_range = [283, 283+int(12.5*60*30)]\n",
    "\n",
    "#     write_frame_difference(folder_path, r\"\\{}_video.mp4\".format(dataset), output_file, frame_range, binning=True)\n",
    "    compute_motion_energy(folder_path, r\"\\{}_video.mp4\".format(dataset), frame_range)\n",
    "    remove_artefacts(folder_path, threshold_lo=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74108250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Multiple files setup\n",
    "setup = False\n",
    "if setup:\n",
    "    general_path = r\"D:\\CVR_dataset\\S9\\Monitoring\"\n",
    "    keyword = [\"RS_M\"]\n",
    "    length = int(10.0 * 60*30)\n",
    "    folders = identify_folders(general_path, keyword)\n",
    "    for path in folders:\n",
    "        print(path)\n",
    "        indices = np.load(path+r\"\\indices.npy\")\n",
    "        start = np.where(np.diff(indices) == 1.0)[0][0]\n",
    "        frame_range = [start, start+length]\n",
    "        \n",
    "        dataset = identify_files(path, \".mp4\")[0][:-10]\n",
    "        output_file = path + r\"\\{}_monitoring.tif\".format(dataset)\n",
    "        \n",
    "        # write_frame_difference(path, r\"\\{}_video.mp4\".format(dataset), output_file, frame_range, binning=True)\n",
    "        compute_motion_energy(path, r\"\\{}_video.mp4\".format(dataset), frame_range)\n",
    "        remove_artefacts(path, threshold_lo=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ccd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\WF_with_cap\\Monitoring\\RS_M49_2_longexpred_video\\monitoring_0.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:44<00:00,  9.04it/s]\n"
     ]
    }
   ],
   "source": [
    "setup = True\n",
    "if setup:\n",
    "    folder_path = r\"D:\\WF_with_cap\\Monitoring\\RS_M49_2_longexpred_video\"\n",
    "    input_file = folder_path + r\"\\RS_M49_2_longexpred_video.mp4\"\n",
    "    output_file = folder_path + r\"\\monitoring.tif\"\n",
    "    frame_range = [0, 400]\n",
    "\n",
    "    convert_to_grayscale(input_file, output_file, frame_range)\n",
    "#     write_frame_difference(folder_path, r\"\\{}_video.mp4\".format(dataset), output_file, frame_range, binning=True)\n",
    "#     compute_motion_energy(folder_path, r\"\\{}_video.mp4\".format(dataset), frame_range)\n",
    "#     remove_artefacts(folder_path, threshold_lo=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d238eb",
   "metadata": {},
   "source": [
    "## Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedf7c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:03<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def time_binning(array):\n",
    "    \"\"\"\n",
    "    Perform time binning on a 3D array of shape (T, M, N).\n",
    "\n",
    "    Parameters:\n",
    "    - array (numpy.ndarray): 3D array with shape (T, M, N).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Binned 3D array with shape (T//5, M, N).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if the input array has the right number of dimensions\n",
    "    if len(array.shape) != 3:\n",
    "        raise ValueError(\"Input array must be 3D\")\n",
    "\n",
    "    T, M, N = array.shape\n",
    "\n",
    "    # If T is not divisible by 5, truncate it to the largest size that is divisible\n",
    "    T_truncated = (T // 5) * 5\n",
    "    array_truncated = array[:T_truncated]\n",
    "\n",
    "    # Reshape the array to group every 5 frames along T axis\n",
    "    reshaped_array = array_truncated.reshape(T_truncated//5, 5, M, N)\n",
    "\n",
    "    # Sum along the second axis (which corresponds to the 5 frames to be binned)\n",
    "    binned_array = reshaped_array.sum(axis=1)\n",
    "\n",
    "    return binned_array\n",
    "\n",
    "path = r\"D:\\Widefield_calcium_imaging\\Data\\dataset_test\\dataset4\"\n",
    "# datasets = [r\"\\IOI_red.tif\", r\"\\IOI_green.tif\", r\"\\dHbO.tif\", r\"\\dHbR.tif\", r\"\\dHbT.tif\"]\n",
    "datasets = [r\"\\IOI_blue.tif\"]\n",
    "for dataset in tqdm(datasets):\n",
    "    data = tf.imread(path+dataset)\n",
    "    data = time_binning(data)\n",
    "    tf.imwrite(path+dataset, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3a12dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter, filtfilt\n",
    "\n",
    "def apply_butterworth_filter(array, lowcut, highcut, fs, order=4):\n",
    "    \"\"\"\n",
    "    Apply a Butterworth filter to a 3D array along axis 0.\n",
    "    \n",
    "    Parameters:\n",
    "        - array: 3D numpy array.\n",
    "        - lowcut: Low cutoff frequency.\n",
    "        - highcut: High cutoff frequency.\n",
    "        - fs: Sampling frequency.\n",
    "        - order: Order of the Butterworth filter (default is 4).\n",
    "        \n",
    "    Returns:\n",
    "        - A 3D numpy array with the filter applied.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Design the Butterworth filter\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    \n",
    "    # Initialize an empty array to store the filtered data\n",
    "    filtered_array = np.empty_like(array)\n",
    "    \n",
    "    # Apply the filter to each (M, N) slice along axis 0\n",
    "    for i in range(array.shape[1]):\n",
    "        for j in range(array.shape[2]):\n",
    "            filtered_array[:, i, j] = filtfilt(b, a, array[:, i, j])\n",
    "            \n",
    "    return filtered_array\n",
    "\n",
    "setup = False\n",
    "if setup:\n",
    "    path = r\"D:\\Widefield_calcium_imaging\\Data\\dataset_test\\dataset1\"\n",
    "    dataset = r\"\\IOI_blue.tif\"\n",
    "    data = tf.imread(path+dataset)\n",
    "    data = apply_butterworth_filter(data, lowcut=0.4, highcut=1.65, fs=3.36, order=4)\n",
    "    tf.imwrite(path+dataset, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply mask\n",
    "def apply_mask(data, mask):\n",
    "    \"\"\"\n",
    "    Apply a 2D binary mask to a 3D array. All values in the 3D array outside of the mask are set to 0.0.\n",
    "    \n",
    "    Parameters:\n",
    "        - data: 3D numpy array of shape (T, M, N).\n",
    "        - mask: 2D binary array of shape (M, N). \n",
    "    \n",
    "    Returns:\n",
    "        - masked_data: 3D numpy array with values outside of the mask set to 0.0.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the binary mask to a boolean mask\n",
    "    bool_mask = mask.astype(bool)\n",
    "    \n",
    "    # Initialize an array filled with zeros\n",
    "    masked_data = np.zeros_like(data)\n",
    "    \n",
    "    # Apply the mask to the 3D data\n",
    "    masked_data[:, bool_mask] = data[:, bool_mask]\n",
    "    \n",
    "    non_zero_coords = np.nonzero(masked_data)\n",
    "    depth_range = (non_zero_coords[0].min(), non_zero_coords[0].max() + 1)\n",
    "    row_range = (non_zero_coords[1].min(), non_zero_coords[1].max() + 1)\n",
    "    col_range = (non_zero_coords[2].min(), non_zero_coords[2].max() + 1)\n",
    "    masked_data = masked_data[depth_range[0]:depth_range[1], row_range[0]:row_range[1], col_range[0]:col_range[1]]\n",
    "    \n",
    "    return masked_data\n",
    "\n",
    "path = r\"D:\\Widefield_calcium_imaging\\Data\\dataset_test\\dataset1\"\n",
    "datasets = [r\"\\IOI_blue.tif\", r\"\\IOI_red.tif\", r\"\\IOI_green.tif\", r\"\\dHbO.tif\", r\"\\dHbR.tif\", r\"\\dHbT.tif\"]\n",
    "mask_path = r\"\\mask.tif\"\n",
    "for dataset in tqdm(datasets):\n",
    "    data = tf.imread(path+dataset)\n",
    "    mask = tf.imread(path+mask_path)\n",
    "    data = apply_mask(data, mask)\n",
    "    tf.imwrite(path+dataset, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd349c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
