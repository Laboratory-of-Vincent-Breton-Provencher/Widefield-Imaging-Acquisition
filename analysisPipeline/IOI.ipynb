{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import find_peaks, correlate, butter, freqs, filtfilt\n",
    "from scipy.ndimage import gaussian_filter, median_filter, shift\n",
    "from scipy.stats import sem\n",
    "from signal_analysis import SignalAnalysis\n",
    "from scipy.optimize import curve_fit\n",
    "# from WFmovie_mod import ioi_get_extinctions, ioi_path_length_factor, ioi_epsilon_pathlength\n",
    "\n",
    "import tifffile as tff\n",
    "import seaborn as sns\n",
    "import seaborn_image as isns\n",
    "import cmcrameri.cm as cmc\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.registration import phase_cross_correlation\n",
    "# import ants\n",
    "import csv\n",
    "\n",
    "# cmap = 'cmc.batlow'\n",
    "# sns.set_palette(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ioi matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_path_length_factor(lambda1, lambda2, npoints):\n",
    "    \"\"\"\n",
    "    Return the pathlength values in cm from Ma. et al., Phil. Trans. R. Soc. B 371: 20150360.\n",
    "    Values are stored in the 'Ma_values.txt' file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lambda1/lambda2: scalar\n",
    "        Wavelengths between which to return pathlength values\n",
    "    npoints: int\n",
    "        Number of sampling points between lambda1/2.\n",
    "    Returns\n",
    "    -------\n",
    "    pathlengths: 1darray\n",
    "        Pathlength values in mm between lambda1 and lambda2\n",
    "    \"\"\"\n",
    "    with open(r\"C:\\Users\\gabri\\Documents\\Université\\Maitrise\\Projet\\Widefield-Imaging-Acquisition\\analysisPipeline\\Ma_values.txt\", 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        ma_values = []\n",
    "        for row in reader:\n",
    "            ma_values.append(list(map(float, row)))\n",
    "    z = np.array(ma_values) #col1: wavelengths, col2: z in mm\n",
    "    z[:,1] = z[:,1]/10 #Convert to cm\n",
    "    if z[0,0] > lambda1:\n",
    "        z = np.concatenate((np.array([[lambda1, 0], [z[0, 0]*0.9999, 0]]), z), axis=0)\n",
    "    if z[-1,0] < lambda2:\n",
    "        z = np.concatenate((z, np.array([[z[-1, 0]*1.00001, 0], [lambda2, 0]])), axis=0)\n",
    "    xi = np.linspace(lambda1, lambda2, npoints)\n",
    "    x = z[:, 0]\n",
    "    pathlengths = z[:, 1]\n",
    "    pathlengths = np.interp(xi, x, pathlengths)\n",
    "    return pathlengths\n",
    "\n",
    "\n",
    "def ioi_get_extinctions(lambda1, lambda2, npoints):\n",
    "    \"\"\"\n",
    "    Returns the extinction coefficients (epsilon) for Hbo and HbR as a function of wavelength between lambda1 and lambda2\n",
    "    Values in 1/(cm*M) by Scott Prahl at http://omlc.ogi.edu/spectra/hemoglobin/index.html are stored in the Prahl_values.txt file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lambda1/lambda2: scalar\n",
    "        Wavelengths between which to return extinction values\n",
    "    npoints: int\n",
    "        Number of sampling points between lambda1/2.\n",
    "    Returns\n",
    "    -------\n",
    "    ext_HbO/HbR: 1darrays\n",
    "        Extinction values for HbO and HbR in 1/(cm*M) between lambda1 and lambda2\n",
    "    \"\"\"\n",
    "    with open(r\"C:\\Users\\gabri\\Documents\\Université\\Maitrise\\Projet\\Widefield-Imaging-Acquisition\\analysisPipeline\\Prahl_values.txt\", 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        prahl_values = []\n",
    "        for row in reader:\n",
    "            prahl_values.append(list(map(float, row)))\n",
    "    E = np.array(prahl_values)\n",
    "    E[:,1:3] = E[:,1:3]*2.303 #correction for neperian form of the B-L law\n",
    "    xi = np.linspace(lambda1, lambda2, npoints)\n",
    "    x = E[:,0]\n",
    "    y_HbO = E[:,1]\n",
    "    y_HbR = E[:,2]\n",
    "    ext_HbO = np.interp(xi, x, y_HbO)\n",
    "    ext_HbR = np.interp(xi, x, y_HbR)\n",
    "    return ext_HbO, ext_HbR\n",
    "\n",
    "\n",
    "def ioi_epsilon_pathlength(lambda1, lambda2, npoints, baseline_hbt, baseline_hbo, baseline_hbr, filter):\n",
    "    \"\"\"\n",
    "    Returns the extinction coefficient*pathlength curve for Hbo and HbR as a function of wavelength between lambda1 and lambda2\n",
    "    Parameters\n",
    "    ---------\n",
    "    lambda1/2: scalars\n",
    "        Wavelengths between which the system specs are defined.\n",
    "    npoints: int\n",
    "        Number of wavelength sampling points in system specs\n",
    "    baseline_hbt/o/r: scalars\n",
    "        Baseline concentrations of HbT, HbO and HbR in the brain, in uM\n",
    "    filter: boolean\n",
    "        Specify if the fluoresence emission filter was in place.\n",
    "    Returns\n",
    "    -------\n",
    "    eps_pathlength: 2darray\n",
    "        2d matrix of the epsilon*pathlength values for both imaging wavelengths (rows) and chromophores (columns) in 1/M.\n",
    "        This matrix is used to solve the modified Beer-Lambert equation for HbO and HbR concentration changes.\n",
    "    \"\"\"\n",
    "    os.chdir(r\"C:\\Users\\gabri\\Documents\\Université\\Maitrise\\Projet\\Widefield-Imaging-Acquisition\")\n",
    "    wl = np.linspace(lambda1, lambda2, npoints)\n",
    "    # c_camera\n",
    "    QE_moment = np.loadtxt(r\"analysisPipeline\\specs sys optique\\QE_moment_5px.csv\", delimiter=';')\n",
    "    p = np.poly1d(np.polyfit(QE_moment[:,0], QE_moment[:,1], 10))\n",
    "    c_camera = p(wl)/np.max(p(wl))\n",
    "    QE_moment, p = None, None\n",
    "    # c_led\n",
    "    FBH530 = np.loadtxt(r\"analysisPipeline\\specs sys optique\\FBH530-10.csv\", skiprows=1, usecols=(0, 2), delimiter=';')\n",
    "    f = interp1d(FBH530[:,0], FBH530[:,1])\n",
    "    c_FBH530 = f(wl)/np.max(f(wl))\n",
    "    FBH630 = np.loadtxt(r\"analysisPipeline\\specs sys optique\\FBH630-10.csv\", skiprows=1, usecols=(0, 2), delimiter=';')\n",
    "    f = interp1d(FBH630[:,0], FBH630[:,1])\n",
    "    c_FBH630 = f(wl)/np.max(f(wl))\n",
    "    c_led = np.array([c_FBH530, c_FBH630])\n",
    "    FBH530, FBH630, c_FBH530, c_FBH630, f = None, None, None, None, None \n",
    "    c_tot = baseline_hbt*10**-6  # Rough baseline concentrations in M\n",
    "    c_pathlength = ioi_path_length_factor(lambda1, lambda2, npoints)\n",
    "    c_ext_hbo, c_ext_hbr = ioi_get_extinctions(lambda1, lambda2, npoints)\n",
    "    # Create vectors of values for the fits\n",
    "    CHbO = baseline_hbo/baseline_hbt*c_tot*np.linspace(0, 1.5, 16) #in M\n",
    "    CHbR = baseline_hbr/baseline_hbt*c_tot*np.linspace(0, 1.5, 16)\n",
    "    # In this computation we neglect the fact that pathlength changes with total concentration\n",
    "    # (it is fixed for a Ctot of 100e-6)\n",
    "    eps_pathlength = np.zeros((2, 2))\n",
    "    IHbO = np.zeros(np.shape(CHbO))\n",
    "    IHbR = np.zeros(np.shape(CHbR))\n",
    "    for iled in range(2):\n",
    "        for iconc in range(len(CHbO)):\n",
    "            IHbO[iconc] = np.sum(c_camera*c_led[iled]*np.exp(-c_ext_hbo*c_pathlength*CHbO[iconc]))\n",
    "            IHbR[iconc] = np.sum(c_camera*c_led[iled]*np.exp(-c_ext_hbr*c_pathlength*CHbR[iconc]))\n",
    "        IHbO = IHbO/np.max(IHbO)\n",
    "        IHbR = IHbR/np.max(IHbR)\n",
    "        # Compute effective eps\n",
    "        # plt.plot(c_camera*c_led[iled]*np.exp(-c_ext_hbr*c_pathlength*CHbO[iconc]), 'r.')\n",
    "        # plt.plot(c_camera*c_led[iled]*np.exp(-c_ext_hbo*c_pathlength*CHbR[iconc]), 'g.')\n",
    "        p1 = np.polyfit(CHbO, -np.log(IHbO), 1)\n",
    "        p2 = np.polyfit(CHbR, -np.log(IHbR), 1)\n",
    "        HbOL = p1[0]\n",
    "        HbRL = p2[0]\n",
    "        eps_pathlength[iled, 0] = HbOL\n",
    "        eps_pathlength[iled, 1] = HbRL\n",
    "    # print(eps_pathlength)\n",
    "    return eps_pathlength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D analysis (time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowpass_filter2D(sig:list, cutoff:float=1, fs:float=10, order:int=5)->list:\n",
    "    \"\"\"lowpass filter easy to use for data\n",
    "\n",
    "    Args:\n",
    "        sig (list): 1D array of data (timeseries, flattenned frames)\n",
    "        cutoff (float): cutoff frequency.  Defaults to 1\n",
    "        fs (float, optional): sampling frequency. Defaults to 10.\n",
    "        order (int, optional): order of the butter filter. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: filtered data\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = cutoff / nyq\n",
    "    b, a = butter(order, low, btype='low')\n",
    "    filtered_data = filtfilt(b, a, sig, axis=0)\n",
    "    return filtered_data\n",
    "    \n",
    "\n",
    "def regress_drift2D(sig:list, time:list)-> list:\n",
    "    \"\"\"Prepares raw data to calculate HbO and HbR: removes \n",
    "        drift if any, and normalizes around 1\n",
    "\n",
    "    Args:\n",
    "        sig (list): 1D array containing signal\n",
    "        time (list): 1D array containing time\n",
    "        \n",
    "    Returns:\n",
    "        list: returns only the signal in a 1D array. Time is the same.\n",
    "    \"\"\"\n",
    "    def droite(x, a, b):\n",
    "        return a*x + b\n",
    "    \n",
    "    popt, pcov = curve_fit(droite, time, sig)\n",
    "    pcov = None\n",
    "    sig_r = sig/droite(time, *popt)\n",
    "\n",
    "    return sig_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepToCompute2D(sig:list, time:list, filter=False, cutoff:float=1, regress=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        sig (list): 1D array of signal (timeseries)\n",
    "        time (list): 1D array of time associated with sig. Must be same length\n",
    "        filter (bool, optional): lowpass filter. Defaults to False.\n",
    "        cutoff (float, optional): cutoff frequency for lowpass filter. Defaults to 1.\n",
    "        regress (bool, optional): linear regression for LED drift. Centers data around 1. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        _type_: 1D array of sig\n",
    "    \"\"\"\n",
    "    if filter:\n",
    "        print(\"Filtering data\")\n",
    "        # sig = lowpass_filter2D(sig, cutoff)\n",
    "        sig = gaussian_filter(sig, sigma=1)\n",
    "\n",
    "    if regress:\n",
    "        print(\"Regressing data\")\n",
    "        sig = regress_drift2D(sig, time)\n",
    "\n",
    "    return sig\n",
    "\n",
    "def convertToHb2D(data_green, data_red):\n",
    "    \"\"\"converts green and red signals to Hb variation in tissue\n",
    "\n",
    "    Args:\n",
    "        data_green (list): preprocessed green timeseries\n",
    "        data_red (list): preprocessed red timeseries\n",
    "\n",
    "    Returns:\n",
    "        list: 2D array (d_HbO, d_HbR) \n",
    "    \"\"\"\n",
    "    lambda1 = 450 #nm\n",
    "    lamba2 = 700 #nm\n",
    "    npoints = 1000\n",
    "    baseline_hbt = 100 #uM\n",
    "    baseline_hbo = 60 #uM\n",
    "    baseline_hbr = 40 #uM\n",
    "    rescaling_factor = 1e6\n",
    "    \n",
    "    eps_pathlength = ioi_epsilon_pathlength(lambda1, lamba2, npoints, baseline_hbt, baseline_hbo, baseline_hbr, filter=None)\n",
    "    Ainv = np.linalg.pinv(eps_pathlength)*rescaling_factor\n",
    "    ln_green = -np.log(data_green.flatten())\n",
    "    ln_red = -np.log(data_red.flatten())\n",
    "    ln_R = np.concatenate((ln_green.reshape(1,len(ln_green)),ln_red.reshape(1,len(ln_green))))\n",
    "    Hbs = np.matmul(Ainv, ln_R)\n",
    "    d_HbO = Hbs[0].reshape(np.shape(data_green))\n",
    "    d_HbR = Hbs[1].reshape(np.shape(data_green))\n",
    "    # Protection against aberrant data points\n",
    "    np.nan_to_num(d_HbO, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.nan_to_num(d_HbR, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return d_HbO, d_HbR\n",
    "\n",
    "\n",
    "def dHb_pipeline2D(data_path, save_path, filter=False, cutoff=2, regress=True):\n",
    "    \"\"\"pipeline to compute Hb from raw data. Data must the sorted first, see data_path arg.\n",
    "\n",
    "    Args:\n",
    "        data_path (string): path of the raw data. Data must be sorted first with the 'splitChannels.py' script, \n",
    "        and timestamps extracted with 'extract_ts_moment.py'\n",
    "        save_path (string): path of folder where computed data must be saved\n",
    "        filter (bool, optional): lowpass time filter. Defaults to False.\n",
    "        cutoff (float, optional): cutoff frequency if filter used. Defaults to 0.2.\n",
    "        regress (bool, optional): linear regression to remove LED drift and center data around 1. Defaults to True.\n",
    "    \"\"\"\n",
    "    # process green\n",
    "    green = np.loadtxt(data_path + \"\\\\csv\\\\530.csv\", skiprows=1, delimiter=',')[:,1]\n",
    "    green_t = np.load(data_path + \"\\\\530ts.npy\")\n",
    "    print(\"Green data loaded\")\n",
    "    green = prepToCompute2D(green, green_t, filter, cutoff, regress)\n",
    "    np.save(data_path + \"\\\\530preped.npy\", green)\n",
    "    green = None\n",
    "    print(\"Green data saved\")\n",
    "\n",
    "    # process red    \n",
    "    red = np.loadtxt(data_path + \"\\\\csv\\\\625.csv\", skiprows=1, delimiter=',')[:,1]\n",
    "    red_t = np.load(data_path + \"\\\\625ts.npy\")\n",
    "    print(\"Red data loaded\")\n",
    "    red = prepToCompute2D(red, red_t, filter, cutoff, regress)\n",
    "    np.save(data_path + \"\\\\625preped.npy\", red)\n",
    "    red = None\n",
    "    print(\"Red data saved\")\n",
    "\n",
    "    # convert to hb\n",
    "    print(\"Convert to dHb\")\n",
    "    green = np.load(data_path + \"\\\\530preped.npy\")\n",
    "    red = np.load(data_path + \"\\\\625preped.npy\")\n",
    "    d_HbO, d_HbR = convertToHb2D(green, red)\n",
    "    Hb = np.array((d_HbO, d_HbR, d_HbO+d_HbR))\n",
    "\n",
    "    # save processed data\n",
    "    np.save(save_path + \"\\\\computedHb.npy\", Hb)\n",
    "    print(\"Done\")\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    data_path = r\"C:\\Users\\gabri\\Desktop\\testAnalyse\\2024_07_18\"\n",
    "    save_path = data_path\n",
    "    dHb_pipeline2D(data_path, save_path, filter=True, regress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D analysis (frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_files(path, keywords):\n",
    "    items = os.listdir(path)\n",
    "    files = []\n",
    "    for item in items:\n",
    "        if all(keyword in item for keyword in keywords):\n",
    "            files.append(item)\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "\n",
    "def create_npy_stack(folder_path:str, save_path:str,  wl:int):\n",
    "    \"\"\"creates a 3D npy stack of raw tiff images\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): folder containing tiff frames\n",
    "        save_path (str): folder to save npy stack\n",
    "        wl (int): wavelength for saved file name\n",
    "    \"\"\"\n",
    "    files = identify_files(folder_path, \"tif\")\n",
    "    # files=files[:250]\n",
    "    for idx, file in tqdm(enumerate(files)):\n",
    "        frame = tff.TiffFile(folder_path+\"\\\\\"+file).asarray()\n",
    "        if idx == 0:\n",
    "            num_frames = len(files)\n",
    "            frame_shape = frame.shape\n",
    "            stack_shape = (num_frames, frame_shape[0], frame_shape[1])\n",
    "            _3d_stack = np.zeros(stack_shape, dtype=np.uint16)\n",
    "        _3d_stack[idx,:,:] = frame\n",
    "\n",
    "    np.save(save_path+\"\\\\{}_rawStack.npy\".format(wl), _3d_stack)\n",
    "\n",
    "\n",
    "def motion_correction(frames):\n",
    "    \"\"\"Applies motion correction based on a phase cross correlation\n",
    "\n",
    "    Args:\n",
    "        frames (_type_): 3D array of frames before correction\n",
    "\n",
    "    Returns:\n",
    "        _type_: 3D array of frames after correction\n",
    "    \"\"\"\n",
    "    fixed_frame = frames[0,:,:]\n",
    "    motion_corrected = np.zeros((frames.shape), dtype=np.uint16)\n",
    "    for idx, frame in tqdm(enumerate(frames)):\n",
    "        if idx == 0:\n",
    "            motion_corrected[0,:,:] = frame\n",
    "            continue\n",
    "        shifted, error, diffphase = phase_cross_correlation(fixed_frame, frame, upsample_factor=10)\n",
    "        corrected_image = shift(frame, shift=(shifted[0], shifted[1]), mode='reflect')\n",
    "        motion_corrected[idx,:,:] = corrected_image\n",
    "    \n",
    "    shifted, error, diffphase, corrected_image, fixed_frame = None, None, None, None, None\n",
    "    return motion_corrected\n",
    "\n",
    "\n",
    "def bin_pixels(frames, bin_size=2):\n",
    "    \"\"\"Bins pixels with bin size\n",
    "\n",
    "    Args:\n",
    "        frames (array): 3D array of frames. \n",
    "        bin_size (int, optional): size of pixel bins. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        array: 3D array, stack of binned data\n",
    "    \"\"\"\n",
    "    for idx, frame in tqdm(enumerate(frames)):\n",
    "        if idx == 0:\n",
    "            height, width = frame.shape[:2]\n",
    "            binned_height = height // bin_size\n",
    "            binned_width = width // bin_size\n",
    "            binned_frames = np.zeros((len(frames), binned_height, binned_width), dtype=np.uint16)\n",
    "\n",
    "        reshaped_frame = frame[:binned_height * bin_size, :binned_width * bin_size].reshape(binned_height, bin_size, binned_width, bin_size)\n",
    "\n",
    "        binned_frame = np.sum(reshaped_frame, axis=(1, 3), dtype=np.float32)\n",
    "        binned_frame = binned_frame / (bin_size**2)\n",
    "        binned_frames[idx,:,:] = binned_frame\n",
    "\n",
    "    height, width, binned_height, binned_width, reshaped_frame = None, None, None, None, None\n",
    "    return binned_frames\n",
    "\n",
    "\n",
    "\n",
    "def lowpass_filter(sig:list, cutoff:float=1, fs:float=10, order:int=5)->list:\n",
    "    \"\"\"lowpass filter easy to use for data\n",
    "\n",
    "    Args:\n",
    "        sig (list): 1D array of data (timeseries, flattenned frames)\n",
    "        cutoff (float): cutoff frequency.  Defaults to 1\n",
    "        fs (float, optional): sampling frequency. Defaults to 10.\n",
    "        order (int, optional): order of the butter filter. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: filtered data\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = cutoff / nyq\n",
    "    b, a = butter(order, low, btype='low')\n",
    "    filtered_data = filtfilt(b, a, sig, axis=0)\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def regress_drift(sig:list, time:list, save_path, wl:int=530)-> list:\n",
    "    \"\"\"Prepares raw data to calculate HbO and HbR: removes \n",
    "        drift if any, and normalizes around 1\n",
    "\n",
    "    Args:\n",
    "        sig (list): 1D array containing signal\n",
    "        time (list): 1D array containing time\n",
    "        wl (int): wavelength of light corresponding to data, necessary for saving data as npy. Defaults to 530\n",
    "        filter (bool): activate Defaults to False\n",
    "        \n",
    "    Returns:\n",
    "        list: returns only the signal in a 1D array. Time is the same.\n",
    "    \"\"\"\n",
    "    def droite(x, a, b):\n",
    "        return a*x + b\n",
    "    \n",
    "    print(\"Global regression\")\n",
    "    popt, pcov = curve_fit(droite, time, sig)\n",
    "    pcov = None\n",
    "    sig_r = sig/droite(time, *popt)\n",
    "\n",
    "    return sig_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepToCompute(frames:list, wl:int=530, correct_motion=False, filter=False, bin_size=None, regress=False):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        frames (list): _description_\n",
    "        wl (int, optional): _description_. Defaults to 530.\n",
    "        correct_motion (bool, optional): _description_. Defaults to False.\n",
    "        filter (bool, optional): _description_. Defaults to False.\n",
    "        bin_size (_type_, optional): _description_. Defaults to None.\n",
    "        regress (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if correct_motion:\n",
    "        print(\"Correcting motion\")\n",
    "        frames = motion_correction(frames)\n",
    "    if bin_size is not None:\n",
    "        print(\"Bining pixels\")\n",
    "        frames = bin_pixels(frames, bin_size=bin_size)\n",
    "    if filter:\n",
    "        print(\"Filtering\")\n",
    "        frames = gaussian_filter(frames, sigma=1, axis=0)\n",
    "    if regress:\n",
    "        print(\"Normalizing\")\n",
    "        frames = frames/np.mean(frames, axis=0)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "def convertToHb(data_green, data_red):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data_green (_type_): _description_\n",
    "        data_red (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    lambda1 = 450 #nm\n",
    "    lamba2 = 700 #nm\n",
    "    npoints = 1000\n",
    "    baseline_hbt = 100 #uM\n",
    "    baseline_hbo = 60 #uM\n",
    "    baseline_hbr = 40 #uM\n",
    "    rescaling_factor = 1e6\n",
    "    \n",
    "    eps_pathlength = ioi_epsilon_pathlength(lambda1, lamba2, npoints, baseline_hbt, baseline_hbo, baseline_hbr, filter=None)\n",
    "    Ainv = (np.linalg.pinv(eps_pathlength)*rescaling_factor).astype(np.int16)\n",
    "    # print(np.min(Ainv), np.max(Ainv))\n",
    "    ln_green = -np.log(data_green.flatten())\n",
    "    ln_red = -np.log(data_red.flatten())\n",
    "    ln_R = np.concatenate((ln_green.reshape(1,len(ln_green)),ln_red.reshape(1,len(ln_green))))\n",
    "    Hbs = np.matmul(Ainv, ln_R).astype(np.int16)\n",
    "    # print(np.min(Hbs), np.max(Hbs))\n",
    "    d_HbO = Hbs[0].reshape(np.shape(data_green))\n",
    "    d_HbR = Hbs[1].reshape(np.shape(data_green))\n",
    "    # Protection against aberrant data points\n",
    "    np.nan_to_num(d_HbO, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.nan_to_num(d_HbR, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return d_HbO, d_HbR\n",
    "\n",
    "\n",
    "def dHb_pipeline(data_path, save_path, correct_motion=True, bin_size=3, filter=True, regress=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data_path (_type_): _description_\n",
    "        save_path (_type_): _description_\n",
    "        correct_motion (bool, optional): _description_. Defaults to True.\n",
    "        bin_size (int, optional): _description_. Defaults to 3.\n",
    "        filter (bool, optional): _description_. Defaults to True.\n",
    "        cutoff (float, optional): _description_. Defaults to 0.2.\n",
    "        regress (bool, optional): _description_. Defaults to True.\n",
    "    \"\"\"\n",
    "    # process green\n",
    "    green = np.load(data_path + \"\\\\530_rawStack.npy\")\n",
    "    print(\"Green data loaded\")\n",
    "    green = prepToCompute(green, wl=530, correct_motion=correct_motion, filter=filter, bin_size=bin_size, regress=regress)\n",
    "    green = np.save(data_path + \"\\\\530preped.npy\", green)\n",
    "    green = None\n",
    "    print(\"Green data preped and saved\")\n",
    "\n",
    "    # process red    \n",
    "    red = np.load(data_path + \"\\\\625_rawStack.npy\")\n",
    "    print(\"Red data loaded\")\n",
    "    red = prepToCompute(green, wl=625, correct_motion=correct_motion, filter=filter, bin_size=bin_size, regress=regress)\n",
    "    red = np.load(data_path + \"\\\\625preped.npy\", red)\n",
    "    red = None\n",
    "    print(\"Red data preped and saved\")\n",
    "\n",
    "    # convert to hb\n",
    "    print(\"Convert to dHb\")\n",
    "    green = np.load(data_path + \"\\\\530preped.npy\")\n",
    "    red = np.load(data_path + \"\\\\625preped.npy\")\n",
    "    d_HbO, d_HbR = convertToHb(green, red)\n",
    "    Hb = np.array((d_HbO, d_HbR, d_HbO+d_HbR))\n",
    "\n",
    "    # save processed data\n",
    "    np.save(save_path + \"\\\\computedHb.npy\", Hb)\n",
    "    print(\"Done\")\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    data_path = r\"Y:\\gGermain\\2024-07-18\"\n",
    "    save_path = r\"Y:\\gGermain\\2024-07-18\"\n",
    "    dHb_pipeline(data_path, save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests saving tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathBase = r\"Y:\\gGermain\\2024-07-18\"\n",
    "\n",
    "# green = np.load(pathBase + \"\\\\530preprocessed.npy\")\n",
    "# red = np.load(pathBase + \"\\\\625preprocessed.npy\")\n",
    "\n",
    "# green = green[:200,:,:]\n",
    "# red = red[:200,:,:]\n",
    "\n",
    "# avg_green = np.mean(green, axis=0)\n",
    "# avg_red = np.mean(red, axis=0)\n",
    "\n",
    "# t_green = green/avg_green\n",
    "# t_red = red/avg_red\n",
    "\n",
    "# d_HbO, d_HbR = convertToHb(t_green, t_red)\n",
    "\n",
    "# plt.imshow(d_HbO[62])\n",
    "\n",
    "# d_HbT = d_HbO + d_HbR\n",
    "\n",
    "# for idx, frame in tqdm(enumerate(d_HbT)):\n",
    "#     im = Image.fromarray(frame, mode='I;16')\n",
    "#     im.save(r\"Y:\\gGermain\\2024-07-18\\tiff_hbt\\test{}.tiff\".format(idx), \"TIFF\")\n",
    "\n",
    "# for idx, frame in tqdm(enumerate(d_HbO)):\n",
    "#     im = Image.fromarray(frame, mode='I;16')\n",
    "#     im.save(r\"Y:\\gGermain\\2024-07-18\\tiff_hbo\\test{}.tiff\".format(idx), \"TIFF\")\n",
    "\n",
    "# for idx, frame in tqdm(enumerate(d_HbR)):\n",
    "#     im = Image.fromarray(frame, mode='I;16')\n",
    "#     im.save(r\"Y:\\gGermain\\2024-07-18\\tiff_hbr\\test{}.tiff\".format(idx), \"TIFF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [00:03, 56.29it/s]\n",
      "200it [00:03, 59.77it/s]\n",
      "200it [00:03, 60.15it/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# green = np.load(r\"Y:\\gGermain\\2024-07-18\\625_rawStack.npy\")\n",
    "# motion_corrected = motion_correction(green)\n",
    "# binned = bin_pixels(motion_corrected)\n",
    "# print(binned.shape)\n",
    "# np.save(\"Y:\\gGermain\\\\2024-07-18\\\\625preprocessed.npy\", binned)\n",
    "\n",
    "# green = np.load(r\"Y:\\gGermain\\\\2024-07-18\\\\530preprocessed.npy\")\n",
    "# red = np.load(r\"Y:\\gGermain\\\\2024-07-18\\\\530preprocessed.npy\")\n",
    "\n",
    "# print(\"converting Hb\")\n",
    "# d_HbO, d_HbR = convertToHb(green, red)\n",
    "# Hb = np.array((d_HbO, d_HbR, d_HbO+d_HbR))\n",
    "\n",
    "# print(\"Saving data\")\n",
    "# np.save(\"Y:\\gGermain\\2024-07-18\\\\computedHb.npy\", Hb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation avec HbO HbR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation = correlate(AP_series, HbO*-1+2e-5)\n",
    "# lag = np.argmax(correlation)\n",
    "# print('lag:', (len(red_t) - lag)*1/10, 's')\n",
    "# plt.plot(correlation)\n",
    "# correlation = correlate(AP_series, HbR*-1+2e-5)\n",
    "# lag = np.argmax(correlation)\n",
    "# print('lag:', (len(red_t) - lag)*1/10, 's')\n",
    "# plt.plot(correlation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
