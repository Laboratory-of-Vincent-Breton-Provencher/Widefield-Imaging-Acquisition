{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script d'analyse d'Alexandre Cléroux. Utilisé comme référence pour générer mes propres scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks, correlate, butter, sosfilt, freqs, filtfilt\n",
    "from scipy.ndimage import gaussian_filter, median_filter, shift, uniform_filter\n",
    "from scipy.stats import sem\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "from skimage.registration import phase_cross_correlation\n",
    "\n",
    "import seaborn as sns\n",
    "import seaborn_image as isns\n",
    "import cmcrameri.cm as cmc\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colormaps as clm\n",
    "\n",
    "from PIL import Image\n",
    "import tifffile as tff\n",
    "\n",
    "cmap = 'cmc.batlow'\n",
    "# sns.set_palette(cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General functions for all modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_files(path, keywords):\n",
    "    items = os.listdir(path)\n",
    "    files = []\n",
    "    for item in items:\n",
    "        if all(keyword in item for keyword in keywords):\n",
    "            files.append(item)\n",
    "    files = [os.path.join(path, f) for f in files]\n",
    "    files.sort(key=lambda x: os.path.getmtime(x))\n",
    "    return files\n",
    "\n",
    "\n",
    "def resample_pixel_value(data, bits):\n",
    "    plage = 2**bits - 1\n",
    "    return (plage * (data - np.min(data))/(np.max(data - np.min(data))))\n",
    "\n",
    "\n",
    "def save_as_tiff(frames, prefixe, save_path):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        frames (array): 3D array of one type of data, ex HbO, HbR, or HbT\n",
    "        prefixe (str): type of data\n",
    "        save_path (str): folder to save data\n",
    "    \"\"\"\n",
    "    for idx, frame in tqdm(enumerate(frames)):\n",
    "        im = Image.fromarray(frame, mode='I;16')\n",
    "        im.save(save_path + \"\\\\{}.tiff\".format(prefixe + str(idx)), \"TIFF\")\n",
    "\n",
    "\n",
    "def create_list_trials(frames_timestamps:list, event_times:list, FPS:int=10): \n",
    "    \"\"\"Creates a list of indices for frames of a same air puff trial, separated for each airpuff, with 3 seconds before, and 10 seconds after the airpuff.\n",
    "\n",
    "    Args:\n",
    "        frames_timestamps (list): array containing the timestamps of all the frames for one channel\n",
    "        event_times (list): array containing the time stamps for the air puff delivery\n",
    "        FPS (int): integer of the framerate or frequency of acquisition for one channel. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        list: a list containing all the indices of the frames, sorted by air puff\n",
    "    \"\"\"\n",
    "\n",
    "    AP_idx = []\n",
    "    for ti in event_times:\n",
    "        AP_idx.append(np.argmin(np.absolute(frames_timestamps-ti)))\n",
    "\n",
    "    AP_idx = AP_idx[:-1]\n",
    "    Nf_bef = 3*FPS\n",
    "    Nf_aft = 10*FPS\n",
    "\n",
    "    sorted_frames_idx = []\n",
    "    for idx in AP_idx:\n",
    "        trial_idx = [idx-Nf_bef, idx+Nf_aft]\n",
    "        sorted_frames_idx.append(trial_idx)\n",
    "\n",
    "    return sorted_frames_idx\n",
    "\n",
    "\n",
    "\n",
    "def create_npy_stack(folder_path:str, save_path:str,  wl:int, saving=False, cutAroundEvent=None):\n",
    "    \"\"\"creates a 3D npy stack of raw tiff images\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): folder containing tiff frames\n",
    "        save_path (str): folder to save npy stack\n",
    "        wl (int): wavelength for saved file name\n",
    "    \"\"\"\n",
    "\n",
    "    if cutAroundEvent is not None:\n",
    "        files = cutAroundEvent\n",
    "    \n",
    "    else:\n",
    "        files = identify_files(folder_path, \"tif\")\n",
    "    \n",
    "    # files=files[:250]\n",
    "    for idx, file in tqdm(enumerate(files)):\n",
    "        frame = tff.TiffFile(file).asarray()\n",
    "        if idx == 0:\n",
    "            num_frames = len(files)\n",
    "            frame_shape = frame.shape\n",
    "            stack_shape = (num_frames, frame_shape[0], frame_shape[1])\n",
    "            _3d_stack = np.zeros(stack_shape, dtype=np.uint16)\n",
    "        _3d_stack[idx,:,:] = frame\n",
    "\n",
    "    if saving:\n",
    "        np.save(save_path+\"\\\\{}_rawStack.npy\".format(wl), _3d_stack)\n",
    "    return _3d_stack\n",
    "\n",
    "\n",
    "def motion_correction(still_frame, frames):\n",
    "    \"\"\"Applies motion correction based on a phase cross correlation\n",
    "\n",
    "    Args:\n",
    "        frames (_type_): 3D array of frames before correction\n",
    "\n",
    "    Returns:\n",
    "        _type_: 3D array of frames after correction\n",
    "    \"\"\"\n",
    "    fixed_frame = still_frame\n",
    "    motion_corrected = np.zeros((frames.shape), dtype=np.uint16)\n",
    "    for idx, frame in tqdm(enumerate(frames)):\n",
    "        if idx == 0:\n",
    "            motion_corrected[0,:,:] = frame\n",
    "            continue\n",
    "        shifted, error, diffphase = phase_cross_correlation(fixed_frame, frame, upsample_factor=10)\n",
    "        corrected_image = shift(frame, shift=(shifted[0], shifted[1]), mode='reflect')\n",
    "        motion_corrected[idx,:,:] = corrected_image\n",
    "    \n",
    "    return motion_corrected\n",
    "\n",
    "\n",
    "def bin_pixels(frames, bin_size=2):\n",
    "    \"\"\"Bins pixels with bin size\n",
    "\n",
    "    Args:\n",
    "        frames (array): 3D array of frames. \n",
    "        bin_size (int, optional): size of pixel bins. Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        array: 3D array, stack of binned data\n",
    "    \"\"\"\n",
    "    for idx, frame in tqdm(enumerate(frames)):\n",
    "        if idx == 0:\n",
    "            height, width = frame.shape[:2]\n",
    "            binned_height = height // bin_size\n",
    "            binned_width = width // bin_size\n",
    "            binned_frames = np.zeros((len(frames), binned_height, binned_width), dtype=np.uint16)\n",
    "\n",
    "        reshaped_frame = frame[:binned_height * bin_size, :binned_width * bin_size].reshape(binned_height, bin_size, binned_width, bin_size)\n",
    "\n",
    "        binned_frame = np.sum(reshaped_frame, axis=(1, 3), dtype=np.float32)\n",
    "        binned_frame = binned_frame / (bin_size**2)\n",
    "        binned_frames[idx,:,:] = binned_frame\n",
    "\n",
    "    return binned_frames\n",
    "\n",
    "\n",
    "def regress_drift(sig:list, time:list, save_path, wl:int=530)-> list:\n",
    "    \"\"\"Prepares raw data to calculate HbO and HbR: removes \n",
    "        drift if any, and normalizes around 1\n",
    "\n",
    "    Args:\n",
    "        sig (list): 1D array containing signal\n",
    "        time (list): 1D array containing time\n",
    "        wl (int): wavelength of light corresponding to data, necessary for saving data as npy. Defaults to 530\n",
    "        filter (bool): activate Defaults to False\n",
    "        \n",
    "    Returns:\n",
    "        list: returns only the signal in a 1D array. Time is the same.\n",
    "    \"\"\"\n",
    "    def droite(x, a, b):\n",
    "        return a*x + b\n",
    "    \n",
    "    print(\"Global regression\")\n",
    "    popt, pcov = curve_fit(droite, time, sig)\n",
    "    pcov = None\n",
    "    sig_r = sig/droite(time, *popt)\n",
    "\n",
    "    return sig_r\n",
    "\n",
    "\n",
    "def prepToCompute(frames:list, correct_motion=False, bin_size=None, regress=False):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        frames (list): _description_\n",
    "        correct_motion (bool, optional): _description_. Defaults to False.\n",
    "        filter (bool, optional): _description_. Defaults to False.\n",
    "        bin_size (_type_, optional): _description_. Defaults to None.\n",
    "        regress (bool, optional): _description_. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    if correct_motion:\n",
    "        print(\"Correcting motion\")\n",
    "        frames = motion_correction(frames[0,:,:], frames)\n",
    "    if bin_size is not None:\n",
    "        print(\"Bining pixels\")\n",
    "        frames = bin_pixels(frames, bin_size=bin_size)\n",
    "    if regress:\n",
    "        print(\"Normalizing\")\n",
    "        frames = frames/np.mean(frames, axis=0)\n",
    "    \n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOI hemodynamic analysis funtions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate IOI matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ioi_path_length_factor(lambda1, lambda2, npoints):\n",
    "    \"\"\"\n",
    "    Return the pathlength values in cm from Ma. et al., Phil. Trans. R. Soc. B 371: 20150360.\n",
    "    Values are stored in the 'Ma_values.txt' file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lambda1/lambda2: scalar\n",
    "        Wavelengths between which to return pathlength values\n",
    "    npoints: int\n",
    "        Number of sampling points between lambda1/2.\n",
    "    Returns\n",
    "    -------\n",
    "    pathlengths: 1darray\n",
    "        Pathlength values in mm between lambda1 and lambda2\n",
    "    \"\"\"\n",
    "    with open(r\"C:\\Users\\gabri\\Documents\\Université\\Maitrise\\Projet\\Widefield-Imaging-Acquisition\\AnalysisPipeline\\reference\\Ma_values.txt\", 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        ma_values = []\n",
    "        for row in reader:\n",
    "            ma_values.append(list(map(float, row)))\n",
    "    z = np.array(ma_values) #col1: wavelengths, col2: z in mm\n",
    "    z[:,1] = z[:,1]/10 #Convert to cm\n",
    "    if z[0,0] > lambda1:\n",
    "        z = np.concatenate((np.array([[lambda1, 0], [z[0, 0]*0.9999, 0]]), z), axis=0)\n",
    "    if z[-1,0] < lambda2:\n",
    "        z = np.concatenate((z, np.array([[z[-1, 0]*1.00001, 0], [lambda2, 0]])), axis=0)\n",
    "    xi = np.linspace(lambda1, lambda2, npoints)\n",
    "    x = z[:, 0]\n",
    "    pathlengths = z[:, 1]\n",
    "    pathlengths = np.interp(xi, x, pathlengths)\n",
    "    return pathlengths\n",
    "\n",
    "\n",
    "def ioi_get_extinctions(lambda1, lambda2, npoints):\n",
    "    \"\"\"\n",
    "    Returns the extinction coefficients (epsilon) for Hbo and HbR as a function of wavelength between lambda1 and lambda2\n",
    "    Values in 1/(cm*M) by Scott Prahl at http://omlc.ogi.edu/spectra/hemoglobin/index.html are stored in the Prahl_values.txt file.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lambda1/lambda2: scalar\n",
    "        Wavelengths between which to return extinction values\n",
    "    npoints: int\n",
    "        Number of sampling points between lambda1/2.\n",
    "    Returns\n",
    "    -------\n",
    "    ext_HbO/HbR: 1darrays\n",
    "        Extinction values for HbO and HbR in 1/(cm*M) between lambda1 and lambda2\n",
    "    \"\"\"\n",
    "    with open(r\"C:\\Users\\gabri\\Documents\\Université\\Maitrise\\Projet\\Widefield-Imaging-Acquisition\\AnalysisPipeline\\reference\\Prahl_values.txt\", 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        prahl_values = []\n",
    "        for row in reader:\n",
    "            prahl_values.append(list(map(float, row)))\n",
    "    E = np.array(prahl_values)\n",
    "    E[:,1:3] = E[:,1:3]*2.303 #correction for neperian form of the B-L law\n",
    "    xi = np.linspace(lambda1, lambda2, npoints)\n",
    "    x = E[:,0]\n",
    "    y_HbO = E[:,1]\n",
    "    y_HbR = E[:,2]\n",
    "    ext_HbO = np.interp(xi, x, y_HbO)\n",
    "    ext_HbR = np.interp(xi, x, y_HbR)\n",
    "    return ext_HbO, ext_HbR\n",
    "\n",
    "\n",
    "def ioi_epsilon_pathlength(lambda1, lambda2, npoints, baseline_hbt, baseline_hbo, baseline_hbr, filter):\n",
    "    \"\"\"\n",
    "    Returns the extinction coefficient*pathlength curve for Hbo and HbR as a function of wavelength between lambda1 and lambda2\n",
    "    Parameters\n",
    "    ---------\n",
    "    lambda1/2: scalars\n",
    "        Wavelengths between which the system specs are defined.\n",
    "    npoints: int\n",
    "        Number of wavelength sampling points in system specs\n",
    "    baseline_hbt/o/r: scalars\n",
    "        Baseline concentrations of HbT, HbO and HbR in the brain, in uM\n",
    "    filter: boolean\n",
    "        Specify if the fluoresence emission filter was in place.\n",
    "    Returns\n",
    "    -------\n",
    "    eps_pathlength: 2darray\n",
    "        2d matrix of the epsilon*pathlength values for both imaging wavelengths (rows) and chromophores (columns) in 1/M.\n",
    "        This matrix is used to solve the modified Beer-Lambert equation for HbO and HbR concentration changes.\n",
    "    \"\"\"\n",
    "    os.chdir(r\"C:\\Users\\gabri\\Documents\\Université\\Maitrise\\Projet\\Widefield-Imaging-Acquisition\")\n",
    "    wl = np.linspace(lambda1, lambda2, npoints)\n",
    "    # c_camera\n",
    "    QE_moment = np.loadtxt(r\"AnalysisPipeline\\specs sys optique\\QE_moment_5px.csv\", delimiter=';')\n",
    "    p = np.poly1d(np.polyfit(QE_moment[:,0], QE_moment[:,1], 10))\n",
    "    c_camera = p(wl)/np.max(p(wl))\n",
    "    QE_moment, p = None, None\n",
    "    # c_led\n",
    "    FBH530 = np.loadtxt(r\"AnalysisPipeline\\specs sys optique\\FBH530-10.csv\", skiprows=1, usecols=(0, 2), delimiter=';')\n",
    "    f = interp1d(FBH530[:,0], FBH530[:,1])\n",
    "    c_FBH530 = f(wl)/np.max(f(wl))\n",
    "    FBH630 = np.loadtxt(r\"AnalysisPipeline\\specs sys optique\\FBH630-10.csv\", skiprows=1, usecols=(0, 2), delimiter=';')\n",
    "    f = interp1d(FBH630[:,0], FBH630[:,1])\n",
    "    c_FBH630 = f(wl)/np.max(f(wl))\n",
    "    c_led = np.array([c_FBH530, c_FBH630])\n",
    "    FBH530, FBH630, c_FBH530, c_FBH630, f = None, None, None, None, None \n",
    "    c_tot = baseline_hbt*10**-6  # Rough baseline concentrations in M\n",
    "    c_pathlength = ioi_path_length_factor(lambda1, lambda2, npoints)\n",
    "    c_ext_hbo, c_ext_hbr = ioi_get_extinctions(lambda1, lambda2, npoints)\n",
    "    # Create vectors of values for the fits\n",
    "    CHbO = baseline_hbo/baseline_hbt*c_tot*np.linspace(0, 1.5, 16) #in M\n",
    "    CHbR = baseline_hbr/baseline_hbt*c_tot*np.linspace(0, 1.5, 16)\n",
    "    # In this computation we neglect the fact that pathlength changes with total concentration\n",
    "    # (it is fixed for a Ctot of 100e-6)\n",
    "    eps_pathlength = np.zeros((2, 2))\n",
    "    IHbO = np.zeros(np.shape(CHbO))\n",
    "    IHbR = np.zeros(np.shape(CHbR))\n",
    "    for iled in range(2):\n",
    "        for iconc in range(len(CHbO)):\n",
    "            IHbO[iconc] = np.sum(c_camera*c_led[iled]*np.exp(-c_ext_hbo*c_pathlength*CHbO[iconc]))\n",
    "            IHbR[iconc] = np.sum(c_camera*c_led[iled]*np.exp(-c_ext_hbr*c_pathlength*CHbR[iconc]))\n",
    "        IHbO = IHbO/np.max(IHbO)\n",
    "        IHbR = IHbR/np.max(IHbR)\n",
    "        # Compute effective eps\n",
    "        # plt.plot(c_camera*c_led[iled]*np.exp(-c_ext_hbr*c_pathlength*CHbO[iconc]), 'r.')\n",
    "        # plt.plot(c_camera*c_led[iled]*np.exp(-c_ext_hbo*c_pathlength*CHbR[iconc]), 'g.')\n",
    "        p1 = np.polyfit(CHbO, -np.log(IHbO), 1)\n",
    "        p2 = np.polyfit(CHbR, -np.log(IHbR), 1)\n",
    "        HbOL = p1[0]\n",
    "        HbRL = p2[0]\n",
    "        eps_pathlength[iled, 0] = HbOL\n",
    "        eps_pathlength[iled, 1] = HbRL\n",
    "    # print(eps_pathlength)\n",
    "    return eps_pathlength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToHb(data_green, data_red):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data_green (_type_): _description_\n",
    "        data_red (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    lambda1 = 450 #nm\n",
    "    lamba2 = 700 #nm\n",
    "    npoints = 1000\n",
    "    baseline_hbt = 100 #uM\n",
    "    baseline_hbo = 60 #uM\n",
    "    baseline_hbr = 40 #uM\n",
    "    rescaling_factor = 1e6\n",
    "    \n",
    "    eps_pathlength = ioi_epsilon_pathlength(lambda1, lamba2, npoints, baseline_hbt, baseline_hbo, baseline_hbr, filter=None)\n",
    "    Ainv = (np.linalg.pinv(eps_pathlength)*rescaling_factor).astype(np.float32)\n",
    "    ln_green = -np.log(data_green.flatten())\n",
    "    ln_red = -np.log(data_red.flatten())\n",
    "    ln_R = np.concatenate((ln_green.reshape(1,len(ln_green)),ln_red.reshape(1,len(ln_green))))\n",
    "    Hbs = np.matmul(Ainv, ln_R).astype(np.float32)\n",
    "    d_HbO = Hbs[0].reshape(np.shape(data_green))\n",
    "    d_HbR = Hbs[1].reshape(np.shape(data_green))\n",
    "    # Protection against aberrant data points\n",
    "    np.nan_to_num(d_HbO, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    np.nan_to_num(d_HbR, copy=False, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    return d_HbO, d_HbR\n",
    "\n",
    "\n",
    "def dHb_pipeline(data_path, save_path, correct_motion=True, bin_size=3, filter=False, regress=True):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data_path (_type_): _description_\n",
    "        save_path (_type_): _description_\n",
    "        correct_motion (bool, optional): _description_. Defaults to True.\n",
    "        bin_size (int, optional): _description_. Defaults to 3.\n",
    "        filter (bool, optional): _description_. Defaults to True.\n",
    "        cutoff (float, optional): _description_. Defaults to 0.2.\n",
    "        regress (bool, optional): _description_. Defaults to True.\n",
    "    \"\"\"\n",
    "    # process green\n",
    "    print(\"Loading green data\")\n",
    "    green = create_npy_stack(data_path + \"\\\\530\", data_path, 530, saving=False)\n",
    "    # green = np.load(data_path + \"\\\\530_rawStack.npy\")\n",
    "    print(\"Green data loaded\")\n",
    "    green = prepToCompute(green, correct_motion=correct_motion, bin_size=bin_size, regress=regress)\n",
    "    green = np.save(data_path + \"\\\\530preped.npy\", green)\n",
    "    green = None\n",
    "    print(\"Green data preped and saved\")\n",
    "\n",
    "    # process red\n",
    "    print(\"Loading red data\")\n",
    "    red = create_npy_stack(data_path + \"\\\\625\", data_path, 625, saving=False)\n",
    "    # red = np.load(data_path + \"\\\\625_rawStack.npy\")\n",
    "    print(\"Red data loaded\")\n",
    "    red = prepToCompute(red, correct_motion=correct_motion, bin_size=bin_size, regress=regress)\n",
    "    red = np.save(data_path + \"\\\\625preped.npy\", red)\n",
    "    red = None\n",
    "    print(\"Red data preped and saved\")\n",
    "\n",
    "    # convert to hb\n",
    "    print(\"Converting to dHb\")\n",
    "    green = np.load(data_path + \"\\\\530preped.npy\")\n",
    "    red = np.load(data_path + \"\\\\625preped.npy\")\n",
    "    d_HbO, d_HbR = convertToHb(green, red)\n",
    "    d_HbT = d_HbO+d_HbR\n",
    "    # resample pixel values\n",
    "    d_HbO = resample_pixel_value(d_HbO, 16).astype(np.uint16)\n",
    "    d_HbR = resample_pixel_value(d_HbR, 16).astype(np.uint16)\n",
    "    d_HbT = resample_pixel_value(d_HbT, 16).astype(np.uint16)\n",
    "    Hb = np.array((d_HbO, d_HbR, d_HbT))\n",
    "    # filter if needed\n",
    "    if filter:\n",
    "        print(\"Filtering\")\n",
    "        Hb = gaussian_filter(Hb, sigma=1, axes=(1))\n",
    "    # save processed data as npy\n",
    "    np.save(save_path + \"\\\\computedHb.npy\", Hb)\n",
    "    # save as tiff\n",
    "    print(\"Saving processed Hb\")\n",
    "    data_types = ['HbO', 'HbR', 'HbT']\n",
    "    for frames, typpe in zip(Hb, data_types):\n",
    "        try:\n",
    "            os.mkdir(save_path + \"\\\\\"  + typpe)\n",
    "        except FileExistsError:\n",
    "            print(\"Folder already created\")\n",
    "        save_as_tiff(frames, typpe, save_path + \"\\\\\" + typpe)\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "\n",
    "def dHb_pipeline_by_trial(data_path, save_path, event_timestamps, correct_motion=True, bin_size=3, filter=True, regress=True):\n",
    "\n",
    "    # open data\n",
    "    print(\"Sorting file names\")\n",
    "    files = [identify_files(data_path + \"\\\\530\", \".tif\"), identify_files(data_path + \"\\\\625\", \".tif\")]   #list:[[green files],[red files]]\n",
    "    ts = [np.load(data_path + \"\\\\530ts.npy\"), np.load(data_path + \"\\\\625ts.npy\")]                        #list:[[green timestamps],[red timestamps]]\n",
    "    sorted_idx_list = [create_list_trials(ts[0], event_timestamps), create_list_trials(ts[1], event_timestamps)]  #list:[green[all trials], red[all trials]]\n",
    "    # print(sorted_idx_list)\n",
    "    for trial_idx in range(len(sorted_idx_list[0])):\n",
    "        # process green\n",
    "        print(\"Procesing green\")\n",
    "        idx_inf = sorted_idx_list[0][trial_idx][0]\n",
    "        idx_sup = sorted_idx_list[0][trial_idx][1]\n",
    "        trial_files = files[0][idx_inf: idx_sup]\n",
    "        green = create_npy_stack(data_path + \"\\\\530\", data_path, 530, saving=False, cutAroundEvent=trial_files)\n",
    "        green = prepToCompute(green, correct_motion=correct_motion, bin_size=bin_size, regress=regress)\n",
    "        \n",
    "        # process red\n",
    "        print(\"Processing red\")\n",
    "        idx_inf = sorted_idx_list[1][trial_idx][0]\n",
    "        idx_sup = sorted_idx_list[1][trial_idx][1]\n",
    "        trial_files = files[1][idx_inf: idx_sup]\n",
    "        red = create_npy_stack(data_path + \"\\\\625\", data_path, 625, saving=False, cutAroundEvent=trial_files)\n",
    "        red = prepToCompute(red, correct_motion=correct_motion, bin_size=bin_size, regress=regress) \n",
    "\n",
    "        # convert to hb\n",
    "        print(\"Converting to dHb\")\n",
    "        d_HbO, d_HbR = convertToHb(green, red)\n",
    "        d_HbT = d_HbO+d_HbR\n",
    "        # resample pixel values\n",
    "        d_HbO = resample_pixel_value(d_HbO, 16).astype(np.uint16)\n",
    "        d_HbR = resample_pixel_value(d_HbR, 16).astype(np.uint16)\n",
    "        d_HbT = resample_pixel_value(d_HbT, 16).astype(np.uint16)\n",
    "        Hb = np.array((d_HbO, d_HbR, d_HbT))\n",
    "        # filter if needed\n",
    "        if filter:\n",
    "            print(\"Filtering\")\n",
    "            Hb = gaussian_filter(Hb, sigma=1, axes=(1))\n",
    "        # save processed data as npy\n",
    "        np.save(save_path + \"\\\\computedHb_trial{}.npy\".format(trial_idx), Hb)\n",
    "        # save as tiff\n",
    "        print(\"Saving processed Hb\")\n",
    "        data_types = ['HbO', 'HbR', 'HbT']\n",
    "        for frames, typpe in zip(Hb, data_types):\n",
    "            try:\n",
    "                os.mkdir(save_path + \"\\\\\"  + typpe)\n",
    "            except FileExistsError:\n",
    "                print(\"Folder already created\")\n",
    "            save_as_tiff(frames, typpe + \"_trial{}_\".format(trial_idx+1), save_path + \"\\\\\" + typpe)\n",
    "\n",
    "\n",
    "\n",
    "AP_times = np.array([  12.01,   35.2 ,   46.51,   74.12,   91.14,  103.63,  114.48,\n",
    "    132.14,  142.77,  169.61,  182.33,  197.83,  209.56,  223.5 ,\n",
    "    239.35,  252.31,  263.77,  279.97,  297.53,  310.62,  323.38,\n",
    "    335.92,  365.67,  383.93,  402.83,  417.51,  430.48,  440.9 ,\n",
    "    456.7 ,  468.25,  480.64])\n",
    "data_path = r\"Z:\\gGermain\\2024-09-17\\3\"\n",
    "dHb_pipeline_by_trial(data_path, data_path, AP_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCaMP analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_deltaFovF(data, data0=None):\n",
    "    if data0 is not None:\n",
    "        data0 = np.mean(data, axis=0)\n",
    "\n",
    "    return (data-data0)/data0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSCI analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_speckle_contrast(raw_speckle_data, window_size=7):\n",
    "    num_frames, height, width = raw_speckle_data.shape\n",
    "    contrast_data = np.zeros((num_frames, height, width), dtype=np.float32)\n",
    "\n",
    "    for frame_idx in tqdm(range(num_frames)):\n",
    "        frame_data = raw_speckle_data[frame_idx]\n",
    "\n",
    "        # Calculate local mean and standard deviation for the current frame\n",
    "        local_mean = uniform_filter(frame_data, size=window_size)\n",
    "        local_variance = uniform_filter(frame_data**2, size=window_size) - local_mean**2\n",
    "        local_std = np.sqrt(local_variance)\n",
    "        \n",
    "        # Calculate speckle contrast for the current frame\n",
    "        contrast_data[frame_idx] = 1/(local_std / local_mean)**2\n",
    "\n",
    "    return contrast_data\n",
    "\n",
    "\n",
    "\n",
    "def LSCI_pipeline(data_path, save_path):\n",
    "    print(\"Loading data\")\n",
    "    data = create_npy_stack(data_path + \"\\\\785\", data_path, wl=785, saving=False)\n",
    "    data = data.astype(np.float32)\n",
    "    print(\"Computing LSCI)\")\n",
    "    data = compute_speckle_contrast(data, window_size=7)\n",
    "    data = resample_pixel_value(data, 16).astype(np.uint16)\n",
    "    print(\"Saving LSCI data\")\n",
    "    try:\n",
    "        os.mkdir(save_path + \"\\\\LSCI\")\n",
    "    except FileExistsError:\n",
    "        print(\"Folder already created\")\n",
    "    save_as_tiff(data, save_path + \"\\\\LSCI\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WFenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
